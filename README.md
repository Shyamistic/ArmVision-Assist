# ArmVision Assist üëÅÔ∏è‚ö°
> **The First Offline-Native, Action-Oriented Vision Agent for ARM Mobile.**

![Status](https://img.shields.io/badge/Status-Production%20Ready-success)
![Platform](https://img.shields.io/badge/Platform-Android%20%7C%20ARM64-blue)
![AI](https://img.shields.io/badge/AI-On--Device-violet)

## üöÄ The Problem
Visually impaired users rely on cloud-based vision tools that are:
1.  **Slow:** Latency kills the experience.
2.  **Privacy-Invasive:** Sending camera feeds to the cloud is risky.
3.  **Passive:** They just "read" text; they don't help you *act* on it.

## üí° The Solution: ArmVision Assist
ArmVision Assist is an **offline AI Agent** optimized for ARM processors. It doesn't just read; it understands.
* **0% Latency:** Runs entirely on-device using ML Kit.
* **Context Aware:** Instantly distinguishes a **Menu** from a **Warning Sign**.
* **Action Engine:** Automatically converts static text into **Clickable Actions** (Call, Email, Open Link).

## üõ†Ô∏è Tech Stack (ARM Optimized)
* **Language:** Kotlin (Native Android)
* **Vision Pipeline:** CameraX + Google ML Kit (Quantized on-device models)
* **Intelligence:** Custom Regex Context Engine
* **Haptics:** Sensory feedback loop for non-visual confirmation.

## üåü Key Features
1.  **Smart HUD:** Corner-bracket tracking with real-time "inference speed" display.
2.  **Safety Guard:** Detects words like "DANGER" or "WARNING" and triggers a priority haptic alert.
3.  **Action Chips:** Scanned a phone number? A "Call" button appears instantly.
4.  **Low-Light Boost:** Auto-detects darkness and engages the flash.

## üì∏ Screenshots
*(Insert your screenshot here - the one with the blue buttons!)*

---
*Built for the ARM Hackathon 2025.*